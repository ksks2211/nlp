{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "model = word2vec.Word2Vec.load(\"movie.model\")\n",
    "normalized_question1 = np.load(open(DATA_IN_PATH + 'normalized_question1.npy', 'rb'),allow_pickle=True)\n",
    "normalized_question2 = np.load(open(DATA_IN_PATH + 'normalized_question2.npy', 'rb'),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what do i use to sterlise my derma roller'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_question1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_features(words, num_features):\n",
    "    global index2word_set,model    \n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    num_words = 0\n",
    "    # 어휘 사전 준비\n",
    "    #index2word_set = set(model.wv.index2word)\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words+=1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model.wv[w])\n",
    "\n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    if(num_words>0) : feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_question1 = [get_features(question,40) for question in normalized_question1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_question2=[get_features(question,40) for question in normalized_question2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "data_configs['vocab'] = model.wv.index2word\n",
    "data_configs['vocab_size'] = len(model.wv.index2word)\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH+'train.csv',encoding='utf-8')\n",
    "\n",
    "TRAIN_Q1_DATA = 'train_q1.npy'\n",
    "TRAIN_Q2_DATA = 'train_q2.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "np.save(open(DATA_IN_PATH + TRAIN_Q1_DATA, 'wb'), vectorized_question1)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_Q2_DATA , 'wb'), vectorized_question2)\n",
    "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + 'test.csv', encoding='utf-8')\n",
    "# test_data = test_data.drop(test_data.tail(1217679).index,inplace=True) # drop last n rows\n",
    "valid_ids = [type(x) ==int for x in test_data.test_id] \n",
    "test_data = test_data[valid_ids].drop_duplicates()\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "change_filter = re.compile(FILTERS)\n",
    "\n",
    "test_questions1 = [str(s) for s in test_data['question1']]\n",
    "test_questions2 = [str(s) for s in test_data['question2']]\n",
    "\n",
    "filtered_test_questions1 = list()\n",
    "filtered_test_questions2 = list()\n",
    "\n",
    "for q in test_questions1:\n",
    "     filtered_test_questions1.append(re.sub(change_filter, \"\", q).lower())\n",
    "        \n",
    "for q in test_questions2:\n",
    "     filtered_test_questions2.append(re.sub(change_filter, \"\", q).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "normalized_test1=[]\n",
    "normalized_test2=[]\n",
    "\n",
    "for str in filtered_test_questions1:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", str.lower())\n",
    "    normalized_test1.append(tokens)\n",
    "for str in filtered_test_questions2:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", str.lower())\n",
    "    normalized_test2.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test1 = [get_features(question,40) for question in normalized_test1]\n",
    "vectorized_test2=[get_features(question,40) for question in normalized_test2]\n",
    "\n",
    "test_id = np.array(test_data['test_id'])\n",
    "\n",
    "TEST_Q1_DATA = 'test_q1.npy'\n",
    "TEST_Q2_DATA = 'test_q2.npy'\n",
    "TEST_ID_DATA = 'test_id.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(DATA_IN_PATH + TEST_Q1_DATA, 'wb'), np.array(vectorized_test1))\n",
    "np.save(open(DATA_IN_PATH + TEST_Q2_DATA , 'wb'), np.array(vectorized_test2))\n",
    "np.save(open(DATA_IN_PATH + TEST_ID_DATA , 'wb'), test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
