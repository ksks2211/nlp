{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "train_data = pd.read_csv(DATA_IN_PATH+'train.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_data = train_data.loc[train_data['is_duplicate']==1]\n",
    "train_neg_data = train_data.loc[train_data['is_duplicate']==0]\n",
    "\n",
    "\n",
    "# 개수의 차이\n",
    "class_difference = len(train_neg_data) - len(train_pos_data)\n",
    "# 많은 집단에서는 일부만 샘플링하기\n",
    "sample_frac = 1 - (class_difference / len(train_neg_data))\n",
    "train_neg_data = train_neg_data.sample(frac = sample_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 질문 개수: 149263\n",
      "중복이 아닌 질문 개수: 149263\n"
     ]
    }
   ],
   "source": [
    "print(\"중복 질문 개수: {}\".format(len(train_pos_data)))\n",
    "print(\"중복이 아닌 질문 개수: {}\".format(len(train_neg_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 다시 합치기\n",
    "train_data = pd.concat([train_neg_data, train_pos_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "MAX_SEQUENCE_LENGTH = 31\n",
    "\n",
    "change_filter = re.compile(FILTERS)\n",
    "questions1 = [str(s) for s in train_data['question1']]\n",
    "questions2 = [str(s) for s in train_data['question2']]\n",
    "filtered_questions1 = list()\n",
    "filtered_questions2 = list()\n",
    "\n",
    "for q in questions1:\n",
    "     filtered_questions1.append(re.sub(change_filter, \"\", q).lower())\n",
    "        \n",
    "for q in questions2:\n",
    "     filtered_questions2.append(re.sub(change_filter, \"\", q).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do i know when to sell a real estate property'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_questions1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "normalized_question1=[]\n",
    "normalized_question2=[]\n",
    "for str in filtered_questions1:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", str.lower())\n",
    "    normalized_question1.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for str in filtered_questions2:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", str.lower())\n",
    "    normalized_question2.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_question1=np.array(normalized_question1)\n",
    "normalized_question2= np.array(normalized_question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(DATA_IN_PATH + 'normalized_question1.npy', 'wb'), normalized_question1)\n",
    "np.save(open(DATA_IN_PATH + 'normalized_question2.npy', 'wb'), normalized_question2)\n",
    "labels = np.array(train_data['is_duplicate'], dtype=int)\n",
    "np.save(open(DATA_IN_PATH + 'train_label.npy', 'wb'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = np.concatenate((normalized_question1,normalized_question2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
