{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_IN_PATH='./data_in/'\n",
    "TRAIN_CLEAN_DATA='train_clean.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH+TRAIN_CLEAN_DATA)\n",
    "\n",
    "reviews = train_data['review']\n",
    "sentiments = train_data['sentiment']\n",
    "sentences =[review.split() for review in reviews ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=300# word vector 특징값의 수\n",
    "min_word_count=40\n",
    "num_workers=4\n",
    "context=10\n",
    "downsampling=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 19:27:32,788 : INFO : collecting all words and their counts\n",
      "2021-05-21 19:27:32,790 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-21 19:27:32,997 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-05-21 19:27:33,210 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-05-21 19:27:33,315 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-05-21 19:27:33,315 : INFO : Loading a fresh vocabulary\n",
      "2021-05-21 19:27:33,362 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-05-21 19:27:33,363 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-05-21 19:27:33,392 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-05-21 19:27:33,395 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-05-21 19:27:33,396 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-05-21 19:27:33,419 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-05-21 19:27:33,420 : INFO : resetting layer weights\n",
      "2021-05-21 19:27:34,721 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-05-21 19:27:35,727 : INFO : EPOCH 1 - PROGRESS: at 33.15% examples, 833797 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:36,734 : INFO : EPOCH 1 - PROGRESS: at 61.28% examples, 766223 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:37,740 : INFO : EPOCH 1 - PROGRESS: at 98.12% examples, 812693 words/s, in_qsize 6, out_qsize 0\n",
      "2021-05-21 19:27:37,756 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-21 19:27:37,763 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-21 19:27:37,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-21 19:27:37,787 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-21 19:27:37,788 : INFO : EPOCH - 1 : training on 2988089 raw words (2494012 effective words) took 3.1s, 814448 effective words/s\n",
      "2021-05-21 19:27:38,802 : INFO : EPOCH 2 - PROGRESS: at 34.78% examples, 867957 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:39,812 : INFO : EPOCH 2 - PROGRESS: at 70.64% examples, 877405 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:40,591 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-21 19:27:40,597 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-21 19:27:40,602 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-21 19:27:40,613 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-21 19:27:40,616 : INFO : EPOCH - 2 : training on 2988089 raw words (2494207 effective words) took 2.8s, 883769 effective words/s\n",
      "2021-05-21 19:27:41,625 : INFO : EPOCH 3 - PROGRESS: at 32.49% examples, 815160 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:42,632 : INFO : EPOCH 3 - PROGRESS: at 68.05% examples, 847728 words/s, in_qsize 8, out_qsize 0\n",
      "2021-05-21 19:27:43,554 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-21 19:27:43,560 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-21 19:27:43,564 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-21 19:27:43,588 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-21 19:27:43,589 : INFO : EPOCH - 3 : training on 2988089 raw words (2494569 effective words) took 3.0s, 840263 effective words/s\n",
      "2021-05-21 19:27:44,607 : INFO : EPOCH 4 - PROGRESS: at 33.82% examples, 841591 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:45,608 : INFO : EPOCH 4 - PROGRESS: at 70.29% examples, 876267 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:46,392 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-21 19:27:46,393 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-21 19:27:46,396 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-21 19:27:46,401 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-21 19:27:46,401 : INFO : EPOCH - 4 : training on 2988089 raw words (2494086 effective words) took 2.8s, 888709 effective words/s\n",
      "2021-05-21 19:27:47,408 : INFO : EPOCH 5 - PROGRESS: at 35.08% examples, 882184 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:48,415 : INFO : EPOCH 5 - PROGRESS: at 69.33% examples, 865478 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-21 19:27:49,222 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-21 19:27:49,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-21 19:27:49,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-21 19:27:49,252 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-21 19:27:49,253 : INFO : EPOCH - 5 : training on 2988089 raw words (2494510 effective words) took 2.8s, 875860 effective words/s\n",
      "2021-05-21 19:27:49,255 : INFO : training on a 14940445 raw words (12471384 effective words) took 14.5s, 858136 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\\\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 19:28:00,791 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2021-05-21 19:28:00,794 : INFO : not storing attribute vectors_norm\n",
      "2021-05-21 19:28:00,795 : INFO : not storing attribute cum_table\n",
      "2021-05-21 19:28:00,957 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name=\"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words,model,num_features):\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float32)   \n",
    "    num_words = 0\n",
    "    \n",
    "    # 어휘사전\n",
    "    index2word_set=set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words+=1\n",
    "            feature_vector = np.add(feature_vector,model.wv[w])\n",
    "    \n",
    "    feature_vector = np.divide(feature_vector,num_words)\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews,model,num_features):\n",
    "    dataset =[ get_features(s,model,num_features) for s in reviews ]\n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vecs = get_dataset(sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=test_data_vecs\n",
    "y=np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED=100\n",
    "TEST_SPLIT=0.2\n",
    "\n",
    "\n",
    "X_train,X_eval, y_train,y_eval = train_test_split(X,y,test_size=TEST_SPLIT,random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lgs=LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8592\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : {}\".format(lgs.score(X_eval,y_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA='test_clean.csv'\n",
    "\n",
    "test_data=pd.read_csv(DATA_IN_PATH+TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences =[review.split() for review in test_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vecs = get_dataset(test_sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_OUT_PATH='./data_out/'\n",
    "\n",
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['id']=test_data['id'].apply(lambda x : x.replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=list(test_data['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dataset=pd.DataFrame({'id':ids,'sentiment':test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH+'lgs_answer.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
